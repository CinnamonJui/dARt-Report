\chapter{討論&結論}
\section{討論}
\subsubsection{使用雲端運算架構原因探討}
在手機處理 AR 的應用已經較消耗手機資源的情況下，將神經網路的部分交由伺服器端運算，使用者的手機不需較高的配置，節省成本。另外，神經網路的運算本身就非常需要 GPU 的加速，即使手機有 GPU，但運算量遠小於伺服器的 GPU，因此為達到更佳的使用者體驗，也是放在雲端運算的原因之一。
當然在雲端運算架構也有缺點，若是使用者在離線的情形下，就無法遊玩此遊戲，不過在現在網路覆蓋率極高，相信在大部分的情況下，不會有這樣的情形發生。相對地，若伺服器端發生故障，則無法使用。
\subsubsection{物件較大影像的類別個數探討}
在網路深度較淺處，圖片較大，影像資源豐富，主要萃取low level的邊緣輪廓特偵，所以使用少量filter就可以完成。反之，深度較深時，經過多次pooling過程，影像大小變小，且影像資訊不斷被精餾萃取，需要使用更大量的filter萃取特偵。原本的Yolo是針對coco資料集做分類，分類數為80類別，所以在backbone的設計上，每層的filter需要更深、更大量。然而，在此專題中，雖然圖片模糊，但是只需要分為4類別，且物件偏大。因此，此實驗利用縮減backbone的方式來達到加速網路及減少浮點運算。越前層的low level feature map萃取的影像主要為基本線條元素，如果縮減破壞的話，對於後面feature map 的再萃取與one by one CONV的分類會影響很大。此專案所需分類數較少，只有4類別。因此，在medium 以及high level的feature map就不需要那麼多去描述紀錄最後要的特偵類別數。

\section{結論}
此系統主要透過 AR 以及神經網路的技術來模擬射飛鏢，首先用特徵點辨識，在實境影像上繪製虛擬標靶、虛擬飛鏢，再利用移動手機來決定飛鏢之方位，並使用手勢判斷發射的速度以及是否發射。手勢判斷是使用 UDP socket 來
手機後相機影像至伺服器端後，影像由佈署於伺服器端的神經網路模組SRCNN和LSTM 來辨識，再回傳飛鏢發射速度。發射過程中，透過世界
坐標系之轉換呈現在實境影像上。
未來希望能將以下幾點加入進模擬的考慮範圍當中，以模擬出更加符合真實狀況。
(1) 更精確的手勢辨識: 現在的樣本是基於組員們的手勢，而我們在運作過程發現仍會因為每個人的手部特徵不同，造成有少數幾位同學發射手勢，卻threashold過低，未來希望能將更多不同的手採納入訓練模型的樣本，更廣泛的辨識手勢。
(2) 手與鏢靶的深度關係: 因為現在所有的虛擬物件是貼在實景的上面，故手會被標靶遮擋，不符合實際狀況，未來希望透過 Computer Vision 的技術，利用手部的顏色以及手部的形狀，將手部貼在影像的最前端，不再被標靶遮擋。